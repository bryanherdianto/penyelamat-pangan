# LLM Deployment - ExecuTorch Migration Summary

## Changes Made

The LLM deployment has been migrated from `llama-cpp-python` to **ExecuTorch** for better integration with Qualcomm hardware and optimized on-device inference.

## Updated Files

### 1. `app/ai/scripts/llm/inference.py`
**Changes:**
- ‚úÖ Replaced `llama_cpp.Llama` with ExecuTorch `_load_for_executorch`
- ‚úÖ Changed model format from `.gguf` to `.pte` (PyTorch ExecuTorch)
- ‚úÖ Added `use_qnn` parameter for Qualcomm QNN backend support
- ‚úÖ Implemented basic tokenization (placeholder - needs actual tokenizer)
- ‚úÖ Updated all methods: `generate()`, `chat()`, `instruct()`
- ‚ö†Ô∏è **Note**: Tokenization is placeholder - needs Falcon3 tokenizer integration

### 2. `app/ai/scripts/llm/deploy.py`
**Changes:**
- ‚úÖ Updated MODEL_PATH to `.pte` format
- ‚úÖ Changed initialization to use `use_qnn` instead of `n_gpu_layers`
- ‚úÖ Updated model info to show "ExecuTorch" backend

### 3. `app/ai/scripts/llm/example_usage.py`
**Changes:**
- ‚úÖ Updated model path to `.pte` format
- ‚úÖ Changed initialization parameters

### 4. `requirements.txt`
**Already Updated:**
- ‚úÖ `executorch` included
- ‚úÖ `torch` and `torchvision` included
- ‚úÖ `onnxruntime-qnn` for ONNX models
- ‚ùå Removed `llama-cpp-python` (no longer needed)

## Model Format Change

| Before | After |
|--------|-------|
| Format: GGUF | Format: PTE (PyTorch ExecuTorch) |
| File: `Falcon3-7B-Instruct-Q4_0.gguf` | File: `falcon3_7b_instruct.pte` |
| Backend: llama.cpp | Backend: ExecuTorch |
| Size: ~4GB (Q4_0) | Size: ~7GB (FP32) or ~3.5GB (INT8) |

## Next Steps Required

### 1. Convert Model to ExecuTorch Format

You need to convert your Falcon3-7B model to `.pte` format. See `EXECUTORCH_CONVERSION_GUIDE.md` for detailed instructions.

**Quick method:**
```python
import torch
from transformers import AutoModelForCausalLM
from executorch.exir import to_edge

model = AutoModelForCausalLM.from_pretrained("tiiuae/falcon-7b-instruct")
model.eval()

# Export
exported_program = torch.export.export(model, ...)
edge_program = to_edge(exported_program)
edge_program.to_executorch().save("falcon3_7b_instruct.pte")
```

### 2. Implement Proper Tokenizer

The current implementation has placeholder tokenization. You need to:

```python
# In inference.py, replace _tokenize and _detokenize methods
from transformers import AutoTokenizer

class Falcon3LLMInference:
    def __init__(self, ...):
        # ... existing code ...
        self.tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-7b-instruct")
    
    def _tokenize(self, text: str) -> torch.Tensor:
        return self.tokenizer.encode(text, return_tensors="pt")
    
    def _detokenize(self, tokens: torch.Tensor) -> str:
        return self.tokenizer.decode(tokens[0], skip_special_tokens=True)
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
pip install transformers sentencepiece
```

### 4. Place Model File

```bash
# Place the converted .pte file in:
app/ai/models/llm/falcon3_7b_instruct.pte
```

### 5. For Qualcomm NPU (Optional)

To use Qualcomm QNN backend:

```python
# In deploy.py, change:
model = Falcon3LLMInference(str(model_path), use_qnn=True, verbose=False)
```

**Requirements:**
- Qualcomm QNN SDK installed
- QNN-optimized .pte model
- Qualcomm NPU hardware

## Benefits of ExecuTorch

### ‚úÖ Advantages:
1. **Hardware Acceleration**: Native Qualcomm NPU support
2. **On-Device**: Optimized for mobile/edge devices
3. **Efficient**: Lower memory footprint with quantization
4. **Integration**: Better PyTorch ecosystem integration
5. **Flexibility**: Easy to add custom ops and optimizations

### ‚ö†Ô∏è Considerations:
1. **Model Conversion**: Requires converting models to .pte format
2. **Tokenizer**: Needs separate tokenizer integration
3. **Streaming**: Not yet fully supported
4. **Documentation**: Newer technology, evolving documentation

## Testing

### Test Inference Directly
```bash
python app/ai/scripts/llm/example_usage.py
```

### Start API Server
```bash
python app/ai/scripts/llm/deploy.py
```

### Run API Tests
```bash
python app/ai/scripts/llm/test-model.py
```

## Troubleshooting

### "Model not found"
- Convert your model to .pte format first
- Place in `app/ai/models/llm/`

### "Error loading ExecuTorch model"
```bash
pip install --upgrade executorch torch
```

### "Tokenization not working"
- Implement proper tokenizer (see step 2 above)
- Install transformers: `pip install transformers`

### "QNN backend not available"
- Install Qualcomm QNN SDK
- Set environment variables
- Use QNN-optimized model

## Architecture Comparison

### Before (llama-cpp-python):
```
User Request ‚Üí FastAPI ‚Üí llama_cpp.Llama ‚Üí GGUF Model ‚Üí Response
```

### After (ExecuTorch):
```
User Request ‚Üí FastAPI ‚Üí ExecuTorch Runtime ‚Üí PTE Model ‚Üí Response
                                    ‚Üì
                          (Optional) QNN Backend ‚Üí Qualcomm NPU
```

## Deployment Ports

- **LSTM API**: Port 8000 (unchanged)
- **LLM API**: Port 8001 (unchanged)

Both APIs can run simultaneously.

## Resources

- **ExecuTorch Conversion Guide**: `EXECUTORCH_CONVERSION_GUIDE.md`
- **LLM Backend Guide**: `LLM_BACKEND_GUIDE.md`
- **ExecuTorch Docs**: https://pytorch.org/executorch/
- **Qualcomm AI Hub**: https://aihub.qualcomm.com/

## Status

- ‚úÖ Code migrated to ExecuTorch
- ‚úÖ API structure maintained
- ‚úÖ Documentation created
- ‚ö†Ô∏è Model conversion required
- ‚ö†Ô∏è Tokenizer integration needed
- üîÑ Ready for testing after model conversion
