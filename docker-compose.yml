services:
  # Ollama service for LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_service
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./ollama-entrypoint.sh:/entrypoint.sh
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    restart: unless-stopped
    # Note: Ollama will run on CPU in Docker
    # For Qualcomm NPU acceleration, run Ollama natively on Windows with QNN support
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # LSTM Food Freshness AI Backend
  lstm_api:
    build:
      context: .
      dockerfile: app/ai/Dockerfile
    container_name: food_freshness_ai
    ports:
      - "8000:8000"
    volumes:
      - ./app/ai:/app
      - ./app/ai/models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

volumes:
  ollama_data:
