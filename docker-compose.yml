services:
  # Ollama service for LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_service
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./ollama-entrypoint.sh:/entrypoint.sh
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    restart: unless-stopped
    # Note: Ollama will run on CPU in Docker
    # For Qualcomm NPU acceleration, run Ollama natively on Windows with QNN support
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # PostgreSQL Database
  postgres:
    image: postgres:16-alpine
    container_name: postgres_db
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=data
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user -d data"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Backend API - Sensor Data Collection
  backend_api:
    build:
      context: .
      dockerfile: app/backend/Dockerfile
    container_name: backend_sensor_api
    ports:
      - "8001:8001"
    volumes:
      - ./app/backend:/app/backend
    environment:
      - PYTHONUNBUFFERED=1
      - DATABASE_URL=postgresql://user:password@postgres:5432/data
      - BLYNK_TOKEN=doDoL-_pRrwBVtx2zXCEyFXLbMOcQQ5E
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped

  # LSTM Food Freshness AI Backend
  lstm_api:
    build:
      context: .
      dockerfile: app/ai/Dockerfile
    container_name: food_freshness_ai
    ports:
      - "8000:8000"
    volumes:
      - ./app/ai:/app
      - ./app/ai/models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

volumes:
  ollama_data:
  postgres_data:
